{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like to the youtube: https://www.youtube.com/watch?v=eCCHDxMaFIk&t=128s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        import chromadb\n",
    "        self.name=''\n",
    "        self.client=chromadb.Client()\n",
    "        self.collection=self.client.get_or_create_collection('db2')\n",
    "        self.index=0\n",
    "\n",
    "    \n",
    "\n",
    "    def read_txt(self,path):\n",
    "        with open(path, 'r', encoding='ISO-8859-1') as file:\n",
    "            data = file.read()\n",
    "\n",
    "    def read_pdf(self,path):\n",
    "        from PyPDF2 import PdfReader\n",
    "        res=''\n",
    "        reader = PdfReader(path)\n",
    "        for page in reader.pages:\n",
    "            res+=page.extract_text()\n",
    "\n",
    "    def read_all_files(self,path):\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                self.read_txt(path+file)\n",
    "            elif file.endswith(\".pdf\"):\n",
    "                self.read_pdf(path+file)\n",
    "            else:\n",
    "                print(\"Not a valid file format\")\n",
    "    \n",
    "    def add(self,path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Enter the path of the folder\")\n",
    "    path=os.getcwd()\n",
    "    data=Data()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab9af969ea147ad970b053d058c917c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8f3471b82d46b8ad204a427bebcae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)dd03daae/config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan.s\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\matan.s\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c364393550ac458d84b76420e7aff455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba2cf32dc8a40bfbad300429376c5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b-python.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " change the world.\n",
      "It’s a fact that has been around for decades, so no news reader on terrestrial media needs explaining just why our collective future depends on it.\n",
      "But what exactly does AI mean? And how can you prepare yourself and your organisation for this future disruption?\n",
      "In this session, we will explore the different types of AI, look at what a smart home looks like and give practical tips for navigating an uncertain landscape where things could get a lot more interesting than just chatting to Siri.\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/CodeLlama-7B-Python-GGUF\", model_file=\"codellama-7b-python.q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat\"\\n# is a noun, document and about are adjectives.\\n# \"cat\" is a noun, which is the subject of the sentence.\\n# This is an adverb, which is the predicate of the sentence.\\n\\n\\nfrom pprint import pprint\\nimport spacy\\nnlp = spacy.load(\\'en_core_web_sm\\')\\ndoc = nlp(\"This is a document about cat\")\\npprint([(token.text, token.pos_) for token in doc])\\n# [(\\'This\\', \\'DET\\'), (\\'is\\', \\'VERB\\'), (\\'a\\', \\'DET\\'), (\\'document\\', \\'NOUN\\'), (\\'about\\', \\'ADV\\'), (\\'cat\\', \\'NOUN\\')]\\ndoc = nlp(\"This is a document about cat\")\\npprint([(token.text, token.pos_, [child for child in token.children]) for token in doc])\\n# [(\\'This\\', \\'DET\\', []), (\\'is\\', \\'VERB\\', [\\'a\\']), (\\'a\\', \\'DET\\', [\\'document\\']), (\\'document\\', \\'NOUN\\', [\\'about\\', \\'cat\\']), (\\'about\\', \\'ADV\\', [\\''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('create a metadata from the folowing sentence:\"This is a document about cat\" \"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
