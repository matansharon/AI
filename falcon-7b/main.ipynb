{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "api_token = os.environ.get(\"huggingfacehub_api_token\")\n",
    "# print(api_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct',model_kwargs={'max_length': 1024,'temperature':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs of 2021, the current president of the United States is Joe Biden.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('who is the president of the united states?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b3e3c9016546729a96cc6e5e555f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24988a3f31124605a0548ecf67d0bd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication result:\n",
      "tensor([[ 100.4863,  -33.1475,  -98.9893,  ...,   62.8583,   -9.8878,\n",
      "           18.5144],\n",
      "        [ -74.9284,   13.3624, -167.2927,  ...,  209.2872, -151.2920,\n",
      "          -34.9081],\n",
      "        [-221.0414,  137.9731,  -90.3802,  ...,   28.1562, -191.0697,\n",
      "          123.6113],\n",
      "        ...,\n",
      "        [  14.1978,   33.4105, -149.9382,  ...,  -25.0527,   88.7485,\n",
      "         -165.1829],\n",
      "        [  40.6845,   39.8332,   21.0677,  ..., -103.4881, -167.7154,\n",
      "           98.4567],\n",
      "        [  14.6025, -174.9495,  223.0302,  ...,   59.3473,  185.3095,\n",
      "            1.1391]])\n",
      "Elapsed time: 4.85 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Set the size of the matrices to multiply\n",
    "matrix_size = 10000\n",
    "\n",
    "# Create two random matrices of the specified size\n",
    "matrix1 = torch.randn(matrix_size, matrix_size)\n",
    "matrix2 = torch.randn(matrix_size, matrix_size)\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Multiply the matrices\n",
    "result = torch.matmul(matrix1, matrix2)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the result and elapsed time\n",
    "print(f\"Matrix multiplication result:\\n{result}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "667b181c630d0c2792b671a6ac5fb5e28e75373882d6e85ca3fd7749e63b4f48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
