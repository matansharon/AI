{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import tempfile\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import commons\n",
    "import utils\n",
    "import argparse\n",
    "import subprocess\n",
    "from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
    "from models import SynthesizerTrn\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "class TextMapper(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.symbols = [x.replace(\"\\n\", \"\") for x in open(vocab_file).readlines()]\n",
    "        self.SPACE_ID = self.symbols.index(\" \")\n",
    "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
    "\n",
    "    def text_to_sequence(self, text, cleaner_names):\n",
    "        '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "        Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "        Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "        '''\n",
    "        sequence = []\n",
    "        clean_text = text.strip()\n",
    "        for symbol in clean_text:\n",
    "            symbol_id = self._symbol_to_id[symbol]\n",
    "            sequence += [symbol_id]\n",
    "        return sequence\n",
    "\n",
    "    def uromanize(self, text, uroman_pl):\n",
    "        iso = \"xxx\"\n",
    "        with tempfile.NamedTemporaryFile() as tf, \\\n",
    "             tempfile.NamedTemporaryFile() as tf2:\n",
    "            with open(tf.name, \"w\") as f:\n",
    "                f.write(\"\\n\".join([text]))\n",
    "            cmd = f\"perl \" + uroman_pl\n",
    "            cmd += f\" -l {iso} \"\n",
    "            cmd +=  f\" < {tf.name} > {tf2.name}\"\n",
    "            os.system(cmd)\n",
    "            outtexts = []\n",
    "            with open(tf2.name) as f:\n",
    "                for line in f:\n",
    "                    line =  re.sub(r\"\\s+\", \" \", line).strip()\n",
    "                    outtexts.append(line)\n",
    "            outtext = outtexts[0]\n",
    "        return outtext\n",
    "\n",
    "    def get_text(self, text, hps):\n",
    "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def filter_oov(self, text):\n",
    "        val_chars = self._symbol_to_id\n",
    "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
    "        print(f\"text after filtering OOV: {txt_filt}\")\n",
    "        return txt_filt\n",
    "\n",
    "def generate():\n",
    "    parser = argparse.ArgumentParser(description='TTS inference')\n",
    "    parser.add_argument('--model-dir', type=str, help='model checkpoint dir')\n",
    "    parser.add_argument('--wav', type=str, help='output wav path')\n",
    "    parser.add_argument('--txt', type=str, help='input text')\n",
    "    parser.add_argument('--uroman-dir', type=str, help='uroman lib dir (will download if not specified)')\n",
    "    args = parser.parse_args()\n",
    "    ckpt_dir, wav_path, txt = args.model_dir, args.wav, args.txt\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Run inference with {device}\")\n",
    "    vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
    "    config_file = f\"{ckpt_dir}/config.json\"\n",
    "    assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
    "    hps = utils.get_hparams_from_file(config_file)\n",
    "    text_mapper = TextMapper(vocab_file)\n",
    "    net_g = SynthesizerTrn(\n",
    "        len(text_mapper.symbols),\n",
    "        hps.data.filter_length // 2 + 1,\n",
    "        hps.train.segment_size // hps.data.hop_length,\n",
    "        **hps.model)\n",
    "    net_g.to(device)\n",
    "    _ = net_g.eval()\n",
    "\n",
    "    g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
    "    print(f\"load {g_pth}\")\n",
    "\n",
    "    _ = utils.load_checkpoint(g_pth, net_g, None)\n",
    "\n",
    "    print(f\"text: {txt}\")\n",
    "    is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n",
    "    if is_uroman:\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            if args.uroman_dir is None:\n",
    "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
    "                print(cmd)\n",
    "                subprocess.check_output(cmd, shell=True)\n",
    "                args.uroman_dir = tmp_dir\n",
    "            uroman_pl = os.path.join(args.uroman_dir, \"bin\", \"uroman.pl\")\n",
    "            print(f\"uromanize\")\n",
    "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
    "            print(f\"uroman text: {txt}\")\n",
    "    txt = txt.lower()\n",
    "    txt = text_mapper.filter_oov(txt)\n",
    "    stn_tst = text_mapper.get_text(txt, hps)\n",
    "    with torch.no_grad():\n",
    "        x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "        hyp = net_g.infer(\n",
    "            x_tst, x_tst_lengths, noise_scale=.667,\n",
    "            noise_scale_w=0.8, length_scale=1.0\n",
    "        )[0][0,0].cpu().float().numpy()\n",
    "\n",
    "    os.makedirs(os.path.dirname(wav_path), exist_ok=True)\n",
    "    print(f\"wav: {wav_path}\")\n",
    "    write(wav_path, hps.data.sampling_rate, hyp)\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
