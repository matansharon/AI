{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa2b4729333473b91fc7bdeea843f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)flash_attn_triton.py:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-chat:\n",
      "- flash_attn_triton.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-chat:\n",
      "- flash_attn_triton.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461b6f1b8cbf4393b9288d31dfc47fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)n/adapt_tokenizer.py:   0%|          | 0.00/1.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-chat:\n",
      "- adapt_tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-chat:\n",
      "- attention.py\n",
      "- adapt_tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d4917579fd4d79873213e32744671b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9e5f39eebb44308359c235f95e0994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2866dbff35694cab972fd7936c0c711a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f1e92c6b0f496eb6c61c1df1d9baa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b56c7904f65447388b7174f385c31ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454b6502d0df4bcbb39b640bfb82d5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "'mosaicml/mpt-7b-chat',\n",
    "trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6101b5205a504b59b6f710aed4b3616f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1592d61fd8534fe58cac6e07b8b9136f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the transformers library\n",
    "# import transformers\n",
    "\n",
    "# Load the smaller model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the model onto the GPU\n",
    "model.to(device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.AutoConfig.from_pretrained(\n",
    "'mosaicml/mpt-7b-chat',\n",
    "trust_remote_code=True\n",
    ")\n",
    "config.attn_config['attn_impl'] = 'triton'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "'mosaicml/mpt-7b-chat',\n",
    "config=config,\n",
    "torch_dtype=torch.bfloat16,\n",
    "trust_remote_code=True\n",
    ")\n",
    "model.to(device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading mosaicml/mpt-7b-chat requires you to execute the modeling file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m config\u001b[39m=\u001b[39mtransformers\u001b[39m.\u001b[39mAutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mmosaicml/mpt-7b-chat\u001b[39m\u001b[39m'\u001b[39m,trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m config\u001b[39m.\u001b[39mattn_config[\u001b[39m'\u001b[39m\u001b[39mattn_impl\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtriton\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mAutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mmosaicml/mpt-7b-chat\u001b[39;49m\u001b[39m'\u001b[39;49m,config\u001b[39m=\u001b[39;49mconfig, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Load the model onto the GPU\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\matan.s\\AppData\\Local\\anaconda3\\envs\\Data\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:453\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[0;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n\u001b[1;32m--> 453\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    454\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m requires you to execute the modeling file in that repo \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mon your local machine. Make sure you have read the code there to avoid malicious use, then set \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         )\n\u001b[0;32m    458\u001b[0m     class_ref \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mauto_map[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m]\n\u001b[0;32m    459\u001b[0m     model_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(\n\u001b[0;32m    460\u001b[0m         class_ref, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    461\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Loading mosaicml/mpt-7b-chat requires you to execute the modeling file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "# Load the model with float32 precision\n",
    "config=transformers.AutoConfig.from_pretrained('mosaicml/mpt-7b-chat',trust_remote_code=True)\n",
    "config.attn_config['attn_impl'] = 'triton'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained('mosaicml/mpt-7b-chat',config=config, torch_dtype=torch.float32)\n",
    "\n",
    "# Load the model onto the GPU\n",
    "model.to(device='cuda:0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "667b181c630d0c2792b671a6ac5fb5e28e75373882d6e85ca3fd7749e63b4f48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
