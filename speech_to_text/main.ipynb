{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "secret_key = os.getenv(\"openai_key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file= open(\"test1.mp3\", \"rb\")\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\",audio_file,api_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"A lot of you have been asking how to do information retrieval in the BlankChain without using OpenAI. So I thought it would be nice to make this video in showing how to use other open source models with BlankChain. So in this video, I'm going to show you how to do information retrieval from text and PDF files without using OpenAI's embeddings, and without using OpenAI's large-limit model. In this video, we will be looking at two applications. First, we will deal with text documents and introduce different concepts and models. And then we will look at how to work with multiple PDF files. So first, we need to install some of the required packages. In this case, we are installing BlankChain, HuggingFaceHub, and SentenceTransformers. Next, we need HuggingFaceHub API token, since we are going to be using models from HuggingFace. In order to get the token, go to your HuggingFace account, click Settings, and then under Settings, you will have these access tokens. You can create a new token or use an existing one if you already have one. Side note, always make sure if you're sharing Google Colab notebooks, always remove your tokens. In one of my previous videos, I forgot to remove my OpenAI API token, and it was a costly mistake. Next, we need some data to work with. So in this case, I am just using state of the unit address, but that is an example provided within the BlankChain library. So what we are doing here is we are importing the request library. So here is the URL of the text file that we are going to read. Then based on the URL, we use request library to get the text file. And we write it to a file named stateoftheunit.txt. When I execute this cell, the file will be stored on my Google Drive. Next, we are using the text loader function, from documents loader, to load our text document. Now, if you run this, we can actually look at the content of our document. Now, we have our document. Next, we want to divide it into smaller chunks, so that we can feed this into our large language models or embeddings. Each model has a specific token size. In this case, we are choosing a token size of 1,000. And you want to make sure that it doesn't exceed the token limit of the model that you're going to be working with. So we're using character text splitter. This will convert a document into chunks of 1,000 tokens, and those are going to be stored in this documents object. There are a total of 42 chunks, and here is the first chunk. Next, we want to compute embeddings from our documents. So in this case, we're using embeddings from HuggingFace, instead of OpenAI's embeddings. Now, this is not the only type of open source embeddings that NandChain supports. There are actually a number of different embeddings that you can use. So here is a list of text embedding models. You have the OpenAI, that is UN and NAMA CPP. Cohere, you can use fake embeddings, and there is instructor embeddings as well. So have a look at these. Depending on the application that you are working on, you want to select specific type of embeddings. Keep in mind that the best embeddings currently available are OpenAI's embeddings, but those are not free. So in this case, I'm simply creating an object of embeddings, right? Now, we need a vector store for information retrieval. So in this case, we are using this FICE CPU. And so you just need to install it. I think I already have it. Okay. There are other vector stores as available in the LinkedIn. You can check out a comprehensive list here. Next, using these embeddings, we need to embed our document. So in this case, we're using from document, pass on our documents or the chunks that we clean, right, and the corresponding embeddings. And this variable is now going to have embedded document. So let's run this. It's essentially a database, a vector store of the embeddings from our document. I have a very detailed video on the topic, so I'm going to put a link to that video. If you're interested, watch that out for more comprehensive coverage. Now, in order to do a query on our documents, so we get the query, calculate its embeddings, and then do a similarity search between the query and the document's embeddings. So, for example, in this case, the query is, what did the president say about the Supreme Court? Using similarity search, it will find the documents that had the text closest to this text. So it came up with a few answers. So, for example, here, Justice Stephen Breyer talking about the United States Supreme Court. Then again here, right? So there's a couple of places based on the similarity search it found. Now, these results are solely based on the similarity search. Now, you can also integrate this with a large language model to do a Q&A based on your text files. Next, I want to show you how you can integrate this as a part of question answer chain using a large language model. So in this case, we're loading the question answer chain from question answering, then we're using the hugging face help, right? The large language model, instead of open AIS model, we are using is the Google LAND D5 Excel, right? So it will get the large language model from hugging face. Next, we need to create a chain. In this case, instead of opening an LLM, I'm going to pass on this LLM that I created. And let's run this. We have our chain. In order to run a query or a prompt, here is what we do. So we have our query, then find, then do the similarity search on the documents based on the verdicts that we provided, and then the chain will simply chain the query and the document and verdicts together to get a response. Let's run this, all right, and here is a response based on the information that we provided. So one of the most serious constitutional responsibility of a president has is nominating someone to serve on the United States Supreme Court. Now this is basically a question answer bot that you can create. Here is just another prompt that I ran using the same chain. So what did the president say about the economy? He said the responses build the economy from the bottom up and middle out, not from the top down. All right, just keep in mind that the responses may not be really accurate because the model that we're using, this is not as good as open AIS models. But it's free and you can experiment with it. Next, I wanted to show you how you can do the same with multiple PDF files. This is going to be a little different than what I had in another video, but I think it will be something interesting. We're going to be using the same basic concepts that we just learned. So first, you need to install the required packages. Next, we import two different functions, so unstructured PDF loader and vector store index created. This is the function where all the magic is going to happen. Next, we are simply connecting a Google Drive to this Google colab.book. So I have a folder within my Google Drive called data underscore two, and it has two different PDF files. The first is this paper on, or the technical report on GPD for all. And then there's another paper related to lip syncing. Next, we are creating loaders to load our files. Now since there are two PDF files, the result will be a list of two loaders. Next, we will look at how do we create vector store. So this function, vector store index created, accepts embeddings. In this case, we are passing on the higher hugging face embeddings that we had already earlier defined. And then we are dividing our documents into different chunks. Right, so a chunk size of 1,000, and then pass on the loaders that we had. So this will basically has embeddings created from these documents. Now if you read this, you would probably seen this warning. Dekron 2 is not installed. That's absolutely fine. You can simply ignore it. It simply affects the speed, but I have found that if I try to install this, it throws an error. So I'm going to ignore this. Next, we need our large language model that we are going to be using for question answer. So in this case, again, I'm using the hugging face hub and getting a Flan T5 model from Google instead of open-ends DaVinci models. For our information retrieval, we are using retrieval with QNA chain. So in this case, we pass on the large language model that we just created. Chain type is stuff, and then the retriever is the index store that we created, but default is using chroma db. We could actually change this, but I think I left it to default. So instead of files, it's going to be using chroma db. And then the input key is going to be question, so that's the prompt or the query that we're going to be passing it on. All right, so let's run this. So now we are ready to run our query. So the way you do it is you have a chain.run and you pass in your query. Let me ask it, how is the GPT-4 onward train? And it says using lower. Now, let's run another one. So let's see, who are the authors of GPT-4 all technical report, okay? And I think it got only the first author. You can play around with the maximum length and stuff so you get better responses. So there you have it. I just wanted to make this quick tutorial on how you can use open source models instead of open AIs models if you don't want to pay. Hope you found this helpful. If you have any questions or comments, please put them in the comment section. I would love to answer them if I can. Thanks for watching. See you in the next one.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(transcript)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "667b181c630d0c2792b671a6ac5fb5e28e75373882d6e85ca3fd7749e63b4f48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
